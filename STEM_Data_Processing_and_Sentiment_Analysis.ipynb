{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "189fWSRvoOIVCnojjrEEYOMgRzN7LpAsz",
      "authorship_tag": "ABX9TyMtdnNfVEjhsQo/NLbpKQMZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bella-ward/Stem-Away-project-files/blob/main/STEM_Data_Processing_and_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vg_-3gWtgcWv"
      },
      "outputs": [],
      "source": [
        "import nltk                                # Python library for NLP\n",
        "import matplotlib.pyplot as plt            # library for visualization\n",
        "import random                              # pseudo-random number generator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from base64 import b64decode\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess raw text for Sentiment analysis"
      ],
      "metadata": {
        "id": "Qua6mlEhgxT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the stopwords from NLTK\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXkxpcaCgjZ-",
        "outputId": "6c410dd0-a23f-4c64-cd52-bc61447997cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
      ],
      "metadata": {
        "id": "7_VBiaLcgjXY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize string:"
      ],
      "metadata": {
        "id": "DMYPmZNphB5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"As a software person (not an engineer but a better than average understanding), I still don’t understand how this system \\\n",
        "works this well. GPT 4 to me seems to have a true understanding of things!!\"\n",
        "\n",
        "# instantiate tokenizer class\n",
        "tokenizer = TweetTokenizer(preserve_case = False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "# tokenize tweets\n",
        "sample_tokens = tokenizer.tokenize(sample)\n",
        "\n",
        "print('String: ')\n",
        "print(sample)\n",
        "print()\n",
        "print('Tokenized string: ')\n",
        "print(sample_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gHS-0QqgjVD",
        "outputId": "0b6156e1-091f-4b94-b304-ce158d92cc92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String: \n",
            "As a software person (not an engineer but a better than average understanding), I still don’t understand how this system works this well. GPT 4 to me seems to have a true understanding of things!!\n",
            "\n",
            "Tokenized string: \n",
            "['as', 'a', 'software', 'person', '(', 'not', 'an', 'engineer', 'but', 'a', 'better', 'than', 'average', 'understanding', ')', ',', 'i', 'still', 'don', '’', 't', 'understand', 'how', 'this', 'system', 'works', 'this', 'well', '.', 'gpt', '4', 'to', 'me', 'seems', 'to', 'have', 'a', 'true', 'understanding', 'of', 'things', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove stop words:"
      ],
      "metadata": {
        "id": "i5FjoiRhhQWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the english stop words list from nltk\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "print(\"Stop words\\n\")\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmgJQhFEgjOR",
        "outputId": "6f70e8ee-add6-4039-d32a-917f948912d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "Punctuation\n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove stop words and punctuation:"
      ],
      "metadata": {
        "id": "dhYySMIPhb5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_tokens)\n",
        "\n",
        "sample_clean = []\n",
        "\n",
        "for word in sample_tokens:\n",
        "    if (word not in stopwords_english and\n",
        "        word not in string.punctuation):\n",
        "        sample_clean.append(word)\n",
        "\n",
        "print('removed stop words and punctuation:')\n",
        "print(sample_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckoUYiZngjC4",
        "outputId": "779d3228-ac04-4012-93a8-b98516cf8386"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['as', 'a', 'software', 'person', '(', 'not', 'an', 'engineer', 'but', 'a', 'better', 'than', 'average', 'understanding', ')', ',', 'i', 'still', 'don', '’', 't', 'understand', 'how', 'this', 'system', 'works', 'this', 'well', '.', 'gpt', '4', 'to', 'me', 'seems', 'to', 'have', 'a', 'true', 'understanding', 'of', 'things', '!', '!']\n",
            "removed stop words and punctuation:\n",
            "['software', 'person', 'engineer', 'better', 'average', 'understanding', 'still', '’', 'understand', 'system', 'works', 'well', 'gpt', '4', 'seems', 'true', 'understanding', 'things']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming ~ converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\n"
      ],
      "metadata": {
        "id": "nRNtUSRZhj1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(sample_clean)\n",
        "\n",
        "# instantiate stemming class\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# create emply list to store the stems\n",
        "sample_stem = []\n",
        "\n",
        "for word in sample_clean:\n",
        "    stem_word = stemmer.stem(word) # stemming word\n",
        "    sample_stem.append(stem_word) # append to the list\n",
        "\n",
        "print('stemmed words: ')\n",
        "print(sample_stem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9V7NyGKgizv",
        "outputId": "3d4b8248-eecf-418d-ed07-6ee78a62398e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "['software', 'person', 'engineer', 'better', 'average', 'understanding', 'still', '’', 'understand', 'system', 'works', 'well', 'gpt', '4', 'seems', 'true', 'understanding', 'things']\n",
            "stemmed words: \n",
            "['softwar', 'person', 'engin', 'better', 'averag', 'understand', 'still', '’', 'understand', 'system', 'work', 'well', 'gpt', '4', 'seem', 'true', 'understand', 'thing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process_sample()"
      ],
      "metadata": {
        "id": "xl3m6LIOhyjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# choose the same tweet\n",
        "tweet = sample\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "\n",
        "    # remove pattern using re\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*','',tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "    # tokenize, lowercase, remove stopwords, and keep stemmer\n",
        "    tokenizer = TweetTokenizer(preserve_case = False, strip_handles = True,\n",
        "                               reduce_len = True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweet_clean = []\n",
        "\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and\n",
        "           word not in string.punctuation):\n",
        "            stemmed_word = stemmer.stem(word)\n",
        "            tweet_clean.append(stemmed_word)\n",
        "\n",
        "    return tweet_clean\n",
        "\n",
        "\n",
        "\n",
        "print()\n",
        "print(tweet)\n",
        "\n",
        "\n",
        "# call the imported function\n",
        "tweets_stem = process_tweet(tweet) # preprocess a given tweet\n",
        "\n",
        "print('preprocessed tweet: ')\n",
        "print(tweets_stem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiqL7xifhzXd",
        "outputId": "f44f12ff-9777-4ada-d093-1969f4320e37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "As a software person (not an engineer but a better than average understanding), I still don’t understand how this system works this well. GPT 4 to me seems to have a true understanding of things!!\n",
            "preprocessed tweet: \n",
            "['softwar', 'person', 'engin', 'better', 'averag', 'understand', 'still', '’', 'understand', 'system', 'work', 'well', 'gpt', '4', 'seem', 'true', 'understand', 'thing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REDDIT DATA"
      ],
      "metadata": {
        "id": "8OD35CeZh90P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "df = pd.read_csv('Copy of Reddit Data 8_23 - r-output.csv')\n",
        "# This will print the first 5 rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "s2QHyKPhhz-U",
        "outputId": "67a95614-76b3-443e-9ac4-9e3935685c1c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5fe3cf96-7fed-42af-bcac-cf52e2e5c652\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5fe3cf96-7fed-42af-bcac-cf52e2e5c652\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Copy of Reddit Data 8_23 - r-output.csv to Copy of Reddit Data 8_23 - r-output (1).csv\n",
            "   Index  Source                                                URL  \\\n",
            "0      1  Reddit  https://www.reddit.com/r/ChatGPT/comments/137v...   \n",
            "1      2  Reddit  https://www.reddit.com/r/ChatGPT/comments/137v...   \n",
            "2      3  Reddit  https://www.reddit.com/r/ChatGPT/comments/137v...   \n",
            "3      4  Reddit  https://www.reddit.com/r/ChatGPT/comments/137v...   \n",
            "4      5  Reddit  https://www.reddit.com/r/ChatGPT/comments/137v...   \n",
            "\n",
            "                       Title Search term  \\\n",
            "0  General discussion thread     ChatGPT   \n",
            "1  General discussion thread     ChatGPT   \n",
            "2  General discussion thread     ChatGPT   \n",
            "3  General discussion thread     ChatGPT   \n",
            "4  General discussion thread     ChatGPT   \n",
            "\n",
            "                                            Question   Question Timestamp  \\\n",
            "0  To discuss anything and everything related to ...  2023-05-04 11:32:08   \n",
            "1  To discuss anything and everything related to ...  2023-05-04 11:32:08   \n",
            "2  To discuss anything and everything related to ...  2023-05-04 11:32:08   \n",
            "3  To discuss anything and everything related to ...  2023-05-04 11:32:08   \n",
            "4  To discuss anything and everything related to ...  2023-05-04 11:32:08   \n",
            "\n",
            "   Question Votes                                             Answer  \\\n",
            "0               0  It seems to be me that now Google/ Bard are th...   \n",
            "1               0  I've used both a lot, and Bard really doesn't ...   \n",
            "2               0  Bard is blocked in my country, any way to acce...   \n",
            "3               0  Literally nothing. AI will replace most of the...   \n",
            "4               0  It’s going to be slow then all of a sudden. Al...   \n",
            "\n",
            "      Answer Timestamp  Answer Votes  Title Sentiment  Question Sentiment  \\\n",
            "0   2023-05-11 6:07:11             0             0.05            0.091667   \n",
            "1  2023-05-11 12:45:17             0             0.05            0.091667   \n",
            "2   2023-05-11 2:12:57             0             0.05            0.091667   \n",
            "3  2023-05-06 10:50:37             0             0.05            0.091667   \n",
            "4  2023-05-07 19:50:04             0             0.05            0.091667   \n",
            "\n",
            "   Answer Sentiment  \n",
            "0          0.106612  \n",
            "1          0.173214  \n",
            "2          0.000000  \n",
            "3          0.600000  \n",
            "4         -0.007143  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article = df.loc[1]['Answer']\n",
        "\n",
        "\n",
        "# instantiate tokenizer class\n",
        "tokenizer = TweetTokenizer(preserve_case = False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "# tokenize tweets\n",
        "summary_tokens = tokenizer.tokenize(article)\n",
        "\n",
        "print()\n",
        "print('Tokenized string: ')\n",
        "print(summary_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jemB2Mblhz5U",
        "outputId": "56516e79-fc56-4748-b203-ebd8fe9bafd9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized string: \n",
            "[\"i've\", 'used', 'both', 'a', 'lot', ',', 'and', 'bard', 'really', \"doesn't\", 'hold', 'a', 'candle', '.', 'google', 'are', 'probably', 'in', 'panic', 'stations', 'right', 'now', 'because', 'i', 'work', 'on', 'macosx', 'for', 'my', 'job', 'and', 'i', 'have', 'done', 'something', 'i', 'would', 'have', 'previously', 'thought', 'to', 'be', 'complete', 'insanity', '(', 'installed', 'edge', 'on', 'a', 'mac', ')', '.', \"i'm\", 'hoping', 'a', 'serious', 'competitor', 'for', 'gpt', '-', '4', 'comes', 'along', 'soon', 'because', 'as', 'it', 'stands', \"it's\", 'just', 'better', 'than', 'google', 'for', 'essentially', 'everything', \"you'd\", 'search', 'for', '.', 'has', 'to', 'be', 'the', 'biggest', 'marketing', 'win', 'for', 'microsoft', 'in', 'a', 'decade', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the english stop words list from nltk\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "print(\"Stop words\\n\")\n",
        "print(stopwords_english)\n",
        "\n",
        "print('\\nPunctuation\\n')\n",
        "print(string.punctuation)\n",
        "\n",
        "##\n",
        "\n",
        "print()\n",
        "print('\\033[92m')\n",
        "print(summary_tokens)\n",
        "print('\\033[94m')\n",
        "\n",
        "summary_clean = []\n",
        "\n",
        "for word in summary_tokens:\n",
        "    if (word not in stopwords_english and\n",
        "        word not in string.punctuation):\n",
        "        summary_clean.append(word)\n",
        "\n",
        "print('removed stop words and punctuation:')\n",
        "print(summary_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlcqdJIphz3O",
        "outputId": "32ae644a-0ef7-4545-9182-9854aaa0ef12"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "Punctuation\n",
            "\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "\n",
            "\u001b[92m\n",
            "[\"i've\", 'used', 'both', 'a', 'lot', ',', 'and', 'bard', 'really', \"doesn't\", 'hold', 'a', 'candle', '.', 'google', 'are', 'probably', 'in', 'panic', 'stations', 'right', 'now', 'because', 'i', 'work', 'on', 'macosx', 'for', 'my', 'job', 'and', 'i', 'have', 'done', 'something', 'i', 'would', 'have', 'previously', 'thought', 'to', 'be', 'complete', 'insanity', '(', 'installed', 'edge', 'on', 'a', 'mac', ')', '.', \"i'm\", 'hoping', 'a', 'serious', 'competitor', 'for', 'gpt', '-', '4', 'comes', 'along', 'soon', 'because', 'as', 'it', 'stands', \"it's\", 'just', 'better', 'than', 'google', 'for', 'essentially', 'everything', \"you'd\", 'search', 'for', '.', 'has', 'to', 'be', 'the', 'biggest', 'marketing', 'win', 'for', 'microsoft', 'in', 'a', 'decade', '.']\n",
            "\u001b[94m\n",
            "removed stop words and punctuation:\n",
            "[\"i've\", 'used', 'lot', 'bard', 'really', 'hold', 'candle', 'google', 'probably', 'panic', 'stations', 'right', 'work', 'macosx', 'job', 'done', 'something', 'would', 'previously', 'thought', 'complete', 'insanity', 'installed', 'edge', 'mac', \"i'm\", 'hoping', 'serious', 'competitor', 'gpt', '4', 'comes', 'along', 'soon', 'stands', 'better', 'google', 'essentially', 'everything', 'search', 'biggest', 'marketing', 'win', 'microsoft', 'decade']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print('\\033[92m')\n",
        "print(summary_clean)\n",
        "print('\\033[94m')\n",
        "\n",
        "# instantiate stemming class\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# create emply list to store the stems\n",
        "summary_stem = []\n",
        "\n",
        "for word in summary_clean:\n",
        "    stem_word = stemmer.stem(word) # stemming word\n",
        "    summary_stem.append(stem_word) # append to the list\n",
        "\n",
        "print('stemmed words: ')\n",
        "print(summary_stem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLcpDKs6hz1K",
        "outputId": "419f6659-c1e5-487e-fe00-bf05adff4554"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[92m\n",
            "[\"i've\", 'used', 'lot', 'bard', 'really', 'hold', 'candle', 'google', 'probably', 'panic', 'stations', 'right', 'work', 'macosx', 'job', 'done', 'something', 'would', 'previously', 'thought', 'complete', 'insanity', 'installed', 'edge', 'mac', \"i'm\", 'hoping', 'serious', 'competitor', 'gpt', '4', 'comes', 'along', 'soon', 'stands', 'better', 'google', 'essentially', 'everything', 'search', 'biggest', 'marketing', 'win', 'microsoft', 'decade']\n",
            "\u001b[94m\n",
            "stemmed words: \n",
            "[\"i'v\", 'use', 'lot', 'bard', 'realli', 'hold', 'candl', 'googl', 'probabl', 'panic', 'station', 'right', 'work', 'macosx', 'job', 'done', 'someth', 'would', 'previous', 'thought', 'complet', 'insan', 'instal', 'edg', 'mac', \"i'm\", 'hope', 'seriou', 'competitor', 'gpt', '4', 'come', 'along', 'soon', 'stand', 'better', 'googl', 'essenti', 'everyth', 'search', 'biggest', 'market', 'win', 'microsoft', 'decad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_article(summary):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        article summary (or title or subtitle)\n",
        "    Output:\n",
        "        summary_clean: a list of words containing the processed summary\n",
        "\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "\n",
        "    # remove hyperlinks\n",
        "    summary = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',summary)\n",
        "\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    summary = re.sub(r'#','',summary)\n",
        "\n",
        "\n",
        "    # tokenize, lowercase, remove stopwords, and keep stemmer\n",
        "    tokenizer = TweetTokenizer(preserve_case = False, strip_handles = True,\n",
        "                               reduce_len = True)\n",
        "    summary_tokens = tokenizer.tokenize(summary)\n",
        "\n",
        "    summary_clean = []\n",
        "\n",
        "    for word in summary_tokens:\n",
        "        if (word not in stopwords_english and\n",
        "           word not in string.punctuation):\n",
        "            stemmed_word = stemmer.stem(word)\n",
        "            summary_clean.append(stemmed_word)\n",
        "\n",
        "    return summary_clean"
      ],
      "metadata": {
        "id": "oiwQ-EhHhzyW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_p = []\n",
        "for i in range (len(df)):\n",
        "  element = df.loc[i]\n",
        "  if pd.notna(element['Title']):\n",
        "    title = process_article(element['Title'])\n",
        "    if pd.notna(element['Question']):\n",
        "      question = process_article(element['Question'])\n",
        "    else:\n",
        "      question = ''\n",
        "    if pd.notna(element['Answer']):\n",
        "      answer = process_article(element['Answer'])\n",
        "    else:\n",
        "      answer = ''\n",
        "\n",
        "    # print('title: ')\n",
        "    # print(title)\n",
        "    # print('question: ')\n",
        "    # print(question)\n",
        "    # print('answer: ')\n",
        "    # print(answer)\n",
        "    df_p.append({\"id\": i, \"processed_text\": question})\n",
        "    df_p.append({\"id\": i, \"processed_text\": answer})\n",
        "dfp = pd.DataFrame(df_p)\n",
        "\n",
        "print(dfp.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUV2qpXHiim8",
        "outputId": "f3e6d5f5-6604-442a-a653-e246d9796086"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                     processed_text\n",
            "0   0  [discuss, anyth, everyth, relat, chatgpt, open...\n",
            "1   0  [seem, googl, bard, new, bing, chatgpt, bing, ...\n",
            "2   1  [discuss, anyth, everyth, relat, chatgpt, open...\n",
            "3   1  [i'v, use, lot, bard, realli, hold, candl, goo...\n",
            "4   2  [discuss, anyth, everyth, relat, chatgpt, open...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_p = []\n",
        "for i in range(len(df)):\n",
        "    element = df.loc[i]\n",
        "    if pd.notna(element['Title']):\n",
        "        title = process_article(element['Title'])\n",
        "        if pd.notna(element['Question']):\n",
        "            question = process_article(element['Question'])\n",
        "        else:\n",
        "            question = ''\n",
        "        if pd.notna(element['Answer']):\n",
        "            answer = process_article(element['Answer'])\n",
        "        else:\n",
        "            answer = ''\n",
        "\n",
        "        df_p.append({\"id\": i, \"processed_question\": question, \"processed_answer\": answer})\n",
        "\n",
        "dfp = pd.DataFrame(df_p)\n",
        "print(dfp.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EITxnHlhimCP",
        "outputId": "f4afba14-c954-4618-fdeb-0a84e6e0430e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                 processed_question  \\\n",
            "0   0  [discuss, anyth, everyth, relat, chatgpt, open...   \n",
            "1   1  [discuss, anyth, everyth, relat, chatgpt, open...   \n",
            "2   2  [discuss, anyth, everyth, relat, chatgpt, open...   \n",
            "3   3  [discuss, anyth, everyth, relat, chatgpt, open...   \n",
            "4   4  [discuss, anyth, everyth, relat, chatgpt, open...   \n",
            "\n",
            "                                    processed_answer  \n",
            "0  [seem, googl, bard, new, bing, chatgpt, bing, ...  \n",
            "1  [i'v, use, lot, bard, realli, hold, candl, goo...  \n",
            "2   [bard, block, countri, way, access, nonetheless]  \n",
            "3  [liter, noth, ai, replac, job, begin, societi,...  \n",
            "4  [’, go, slow, sudden, job, make, commerci, tv,...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfp.to_csv('reddit-processed-text.csv', index=False)\n",
        "# ! cat r-output.csv"
      ],
      "metadata": {
        "id": "IawpFcmfioi4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis based on question:"
      ],
      "metadata": {
        "id": "hEf7a2ayitND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Fill missing values with empty strings\n",
        "df[['Title', 'Question', 'Answer']] = df[['Title', 'Question', 'Answer']].fillna('')\n",
        "\n",
        "# Calculate sentiment polarity for 'Title', 'Question', and 'Answer'\n",
        "df['Title Sentiment'] = df['Title'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "df['Question Sentiment'] = df['Question'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "df['Answer Sentiment'] = df['Answer'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "question_sentiment_array_textblob = df['Question Sentiment'].to_numpy()\n",
        "df[['Question', 'Question Sentiment']]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "HxQYXP_sipEu",
        "outputId": "b8714c62-e038-45ef-f070-36dea98da57b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Question  Question Sentiment\n",
              "0     To discuss anything and everything related to ...            0.091667\n",
              "1     To discuss anything and everything related to ...            0.091667\n",
              "2     To discuss anything and everything related to ...            0.091667\n",
              "3     To discuss anything and everything related to ...            0.091667\n",
              "4     To discuss anything and everything related to ...            0.091667\n",
              "...                                                 ...                 ...\n",
              "2335                                                               0.000000\n",
              "2336                                                               0.000000\n",
              "2337                                                               0.000000\n",
              "2338                                                               0.000000\n",
              "2339                                                               0.000000\n",
              "\n",
              "[2340 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-de7453b1-c570-44b0-ac36-d05db2acdc4f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Question Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>To discuss anything and everything related to ...</td>\n",
              "      <td>0.091667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>To discuss anything and everything related to ...</td>\n",
              "      <td>0.091667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>To discuss anything and everything related to ...</td>\n",
              "      <td>0.091667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>To discuss anything and everything related to ...</td>\n",
              "      <td>0.091667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>To discuss anything and everything related to ...</td>\n",
              "      <td>0.091667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2335</th>\n",
              "      <td></td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2336</th>\n",
              "      <td></td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2337</th>\n",
              "      <td></td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2338</th>\n",
              "      <td></td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2339</th>\n",
              "      <td></td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2340 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de7453b1-c570-44b0-ac36-d05db2acdc4f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-de7453b1-c570-44b0-ac36-d05db2acdc4f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-de7453b1-c570-44b0-ac36-d05db2acdc4f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9acaf1c7-5a92-4e21-aac9-581b161b145a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9acaf1c7-5a92-4e21-aac9-581b161b145a')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9acaf1c7-5a92-4e21-aac9-581b161b145a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "# Download the VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the VADER sentiment intensity analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Fill missing values with empty strings\n",
        "df[['Title', 'Question', 'Answer']] = df[['Title', 'Question', 'Answer']].fillna('')\n",
        "\n",
        "# Calculate sentiment polarity for 'Title', 'Question', and 'Answer'\n",
        "df['Question Sentiment'] = df['Question'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "# Convert the pandas Series to a numpy array\n",
        "question_sentiment_array_vader = df['Question Sentiment'].to_numpy()\n",
        "print(df[['Question', 'Question Sentiment']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8BbV3sfipCG",
        "outputId": "0fb86587-7a8d-47b2-d748-aca5d5079c6f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Question  Question Sentiment\n",
            "0     To discuss anything and everything related to ...              0.7184\n",
            "1     To discuss anything and everything related to ...              0.7184\n",
            "2     To discuss anything and everything related to ...              0.7184\n",
            "3     To discuss anything and everything related to ...              0.7184\n",
            "4     To discuss anything and everything related to ...              0.7184\n",
            "...                                                 ...                 ...\n",
            "2335                                                                 0.0000\n",
            "2336                                                                 0.0000\n",
            "2337                                                                 0.0000\n",
            "2338                                                                 0.0000\n",
            "2339                                                                 0.0000\n",
            "\n",
            "[2340 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save CSV"
      ],
      "metadata": {
        "id": "WVSLHnKTi3jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('r-output.csv', index=False)\n",
        "# ! cat r-output.csv\n"
      ],
      "metadata": {
        "id": "TPMuEz7OipAD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the difference between the two methods\n",
        "difference = question_sentiment_array_textblob - question_sentiment_array_vader\n",
        "# Compute the mean difference\n",
        "mean_difference = np.mean(difference)\n",
        "\n",
        "# Compute the absolute difference\n",
        "absolute_difference = np.abs(difference)\n",
        "\n",
        "# Compute the mean absolute difference\n",
        "mean_absolute_difference = np.mean(absolute_difference)\n",
        "\n",
        "print(\"Mean Difference:\", mean_difference)\n",
        "print(\"Mean Absolute Difference:\", mean_absolute_difference)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAoJtfVnio9b",
        "outputId": "57fafd8d-733e-431e-fc31-b27671002b1c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Difference: 0.0027770711743641944\n",
            "Mean Absolute Difference: 0.10092488524528455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with text and sentiment scores\n",
        "df_sentiment = pd.DataFrame({\n",
        "    'Text': df['Question'],\n",
        "    'TextBlob': question_sentiment_array_textblob,\n",
        "    'VADER': question_sentiment_array_vader\n",
        "})\n",
        "\n",
        "# Compute the difference and absolute difference\n",
        "df_sentiment['Difference'] = df_sentiment['TextBlob'] - df_sentiment['VADER']\n",
        "df_sentiment['Absolute Difference'] = np.abs(df_sentiment['Difference'])\n",
        "\n",
        "# Sort by absolute difference in descending order\n",
        "df_sentiment = df_sentiment.sort_values(by='Absolute Difference', ascending=False)\n",
        "\n",
        "# Print the 5 entries with the highest absolute difference\n",
        "print(df_sentiment.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38MRyHFxio4f",
        "outputId": "58039d19-9ca1-4a4d-ec39-cb90c803971d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  Text  TextBlob   VADER  \\\n",
            "99   I was having a conversation about neural netwo... -0.047047 -0.9996   \n",
            "111  I was having a conversation about neural netwo... -0.047047 -0.9996   \n",
            "100  I was having a conversation about neural netwo... -0.047047 -0.9996   \n",
            "101  I was having a conversation about neural netwo... -0.047047 -0.9996   \n",
            "102  I was having a conversation about neural netwo... -0.047047 -0.9996   \n",
            "\n",
            "     Difference  Absolute Difference  \n",
            "99     0.952553             0.952553  \n",
            "111    0.952553             0.952553  \n",
            "100    0.952553             0.952553  \n",
            "101    0.952553             0.952553  \n",
            "102    0.952553             0.952553  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jog6v85CjIJD",
        "outputId": "08a4f11b-a8cd-458c-a59a-0ddceca8239b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.3\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl (211.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.23.5)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->xformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->xformers) (1.3.0)\n",
            "Installing collected packages: xformers\n",
            "Successfully installed xformers-0.0.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "CZJINp83jIAJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Word2Vec model\n",
        "w2v_model = Word2Vec(dfp['processed_text'], min_count=1, vector_size=100, workers=4)\n",
        "\n",
        "# Function to vectorize a sentence\n",
        "def vectorize_sentence(sentence, model):\n",
        "    vec = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    return np.mean(vec, axis=0) if vec else np.zeros(model.vector_size)\n",
        "\n",
        "# Vectorize each article summary\n",
        "dfp['vec'] = dfp['processed_text'].apply(lambda x: vectorize_sentence(x, w2v_model))\n",
        "\n",
        "# Calculate the cosine similarity matrix\n",
        "similarity_matrix = cosine_similarity(np.vstack(dfp['vec'].values))\n",
        "\n",
        "# Construct a reverse map of indices and article titles\n",
        "indices = pd.Series(dfp.index, index=dfp['id']).drop_duplicates()\n",
        "\n",
        "\n",
        "print('w2v_model: ... ')\n",
        "print(w2v_model)\n",
        "print('dfp[vec]: ... ')\n",
        "print(dfp['vec'])\n",
        "print('similarity_matrix: ... ')\n",
        "print(similarity_matrix)\n",
        "print('indices: ... ')\n",
        "print(indices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "Etw-WO6KjH9j",
        "outputId": "cb0750d8-7cba-4081-b1dc-04cb4e99efb8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'processed_text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a7a9a5bc3536>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train a Word2Vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Function to vectorize a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvectorize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'processed_text'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reccommendation system from - Anya"
      ],
      "metadata": {
        "id": "vIXPChdXjS4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "\n",
        "# Train a Word2Vec model\n",
        "w2v_model = Word2Vec(dfp['processed_text'], min_count=1, vector_size=100, workers=4)\n",
        "\n",
        "# Function to vectorize a sentence\n",
        "def vectorize_sentence(sentence, model):\n",
        "    vec = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    return np.mean(vec, axis=0) if vec else np.zeros(model.vector_size)\n",
        "\n",
        "# Vectorize each article summary\n",
        "dfp['vec'] = dfp['processed_text'].apply(lambda x: vectorize_sentence(x, w2v_model))\n",
        "\n",
        "# Calculate the cosine similarity matrix\n",
        "similarity_matrix = cosine_similarity(np.vstack(dfp['vec'].values))\n",
        "\n",
        "# Construct a reverse map of indices and article titles\n",
        "indices = pd.Series(dfp.index, index=dfp['id']).drop_duplicates()\n",
        "\n",
        "# Define a recommendation function\n",
        "def get_recommendations(id, cosine_sim=similarity_matrix):\n",
        "    idx = indices[id]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    # sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:11]  # Get scores of the 10 most similar articles\n",
        "    article_indices = [i[0] for i in sim_scores]\n",
        "    return dfp['id'].iloc[article_indices]\n",
        "recommendations = get_recommendations(487)\n",
        "print(dfp['processed_text'][487])\n",
        "print(recommendations)\n",
        "for recommendation in recommendations:\n",
        "  print(dfp['processed_text'][recommendation])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "HxzrLE5MjH6t",
        "outputId": "e1510ed5-6e02-464f-867c-ef63fdd5288c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'processed_text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-fd627963f8f7>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train a Word2Vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Function to vectorize a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'processed_text'"
          ]
        }
      ]
    }
  ]
}